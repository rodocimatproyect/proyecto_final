# -*- coding: utf-8 -*-
"""Proyecto_Final_CD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T_Wgz-QUbUgkXaxovANkU3GkDbpdQ373
"""

!pip install emoji

!pip install gensim

"""# M√≥dulos"""

# M√≥dulos para tratamiento de Datos
import pandas as pd
import numpy as np
pd.set_option('display.max_colwidth', None)

# M√≥dulos para Gr√°ficas
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(
    style="whitegrid",   # fondo claro con l√≠neas suaves
    rc={
        'axes.edgecolor': '0.2',   # bordes m√°s suaves
        'grid.color': '0.85',      # grid gris clarito
        'axes.labelcolor': '0.1',
        'text.color': '0.1',
        'xtick.color': '0.2',
        'ytick.color': '0.2'
    }
)
sns.set_palette("Greys_r")
plt.style.use('fivethirtyeight')
from wordcloud import WordCloud

# sklearn,modelos y m√©tricas
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# m√≥dulos para tratamiento de texto
import re
import string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

#modulos espec√≠ficos
import emoji #para desmojizar
from gensim.models import Word2Vec

# modulos para la red recurrente
from keras.models import Sequential, load_model
from keras.layers import Dense, LSTM, Bidirectional,Embedding, Dropout, Conv1D, MaxPooling1D, GlobalMaxPool1D, SpatialDropout1D
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import plot_model
from keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import EarlyStopping

"""# Funciones"""

stop_words = set(stopwords.words('english'))

def plot_confusion_matrix(model, X_test, y_test):


    y_pred = model.predict(X_test)


    y_true = np.argmax(np.array(y_test), axis=1)
    y_pred_labels = np.argmax(y_pred, axis=1)


    cm = confusion_matrix(y_true, y_pred_labels)

    class_names = ["negativo", "neutral", "positivo"]


    plt.figure(figsize=(8,6))
    sns.heatmap(
        cm,
        cmap=plt.cm.Blues,
        annot=True,
        fmt='d',
        xticklabels=class_names,
        yticklabels=class_names
    )

    plt.title('Confusion Matrix - LSTM', fontsize=16)
    plt.xlabel('Predicted label', fontsize=12)
    plt.ylabel('True label', fontsize=12)
    plt.show()


def convert_emojis(text):
    # Cada emoji se convierte en: " EMOJI_nombre_del_emoji "
    return emoji.demojize(
        text,
        language="en",
        delimiters=(" EMOJI_", " ")
    )

def clean_text(text):
    # 0. Convertir emojis primero
    text = convert_emojis(text)  # usa la funci√≥n de arriba

    # 1. Min√∫sculas
    text = text.lower()

    # 2. Quitar menciones
    text = re.sub(r'@\w+', ' ', text)

    # 3. Quitar URLs
    text = re.sub(r'http\S+|www\.\S+', ' ', text)

    # 4. Quitar puntos suspensivos y comillas raras
    text = text.replace('‚Ä¶', ' ')
    text = text.replace('‚Äú', ' ')
    text = text.replace('‚Äù', ' ')
    text = text.replace('‚Äô', ' ')

    # 5. Quitar n√∫meros
    text = re.sub(r'\d+', ' ', text)

    # 6. Quitar signos de puntuaci√≥n, pero dejando guiones bajos
    #    (porque nuestros tokens tienen "_")
    punct = string.punctuation.replace("_", "")
    text = text.translate(str.maketrans({p: " " for p in punct}))

    # 7. Quitar espacios extra
    text = re.sub(r'\s+', ' ', text).strip()

    # 8. quitar stopwords pero conservando emojis
    tokens = text.split()
    cleaned_tokens = []
    for tok in tokens:
        # si es emoji_..., lo dejamos tal cual
        if tok.startswith("emoji_"):
            cleaned_tokens.append(tok)
            continue

        # quitar stopwords
        if tok in stop_words:
            continue

        # lematizar palabra normal
        lemma = lemmatizer.lemmatize(tok)   # por defecto asume sustantivo
        cleaned_tokens.append(lemma)

    return " ".join(cleaned_tokens)

max_words = 10000
max_len = 50

def tokenize_pad_sequences(text):
    '''
    Esta funci√≥n aplica tokenizaci√≥n y padding
    '''
    # Tokenizacion
    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')
    tokenizer.fit_on_texts(text)

    X = tokenizer.texts_to_sequences(text)
    # hacemos padding
    X = pad_sequences(X, padding='post', maxlen=max_len)

    return X, tokenizer

def tokenize_valid_set(texts, tokenizer, max_len = max_len):
    seq = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')
    return padded

etiquetas = {'negative' : 0, 'neutral': 1, 'positive':2}

# --- Funci√≥n para crear un WordCloud ---
def create_wc(text):
    return WordCloud(
        width=900,
        height=450,
        background_color="white",
        colormap="Greys",
        max_words=120
    ).generate(text)

"""# Carga de Datos"""

df = pd.read_csv('/content/Tweets.csv')

df['airline'].unique()

cols = ['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence', 'airline', 'text', 'tweet_created', 'negativereason']
df = df[cols]

"""# Tratamiento"""

# Ejemplo para ver como limpia un tweet que trae un emoji
mask_emoji = df['text'].astype(str).str.contains("üòÖ", na=False)
df_with_emoji = df[mask_emoji]

df_with_emoji['text'].values[0]

clean_text(df_with_emoji['text'].iloc[0])

#limpiamos_todo
df['final_text'] = df['text'].apply(clean_text)

# extraemos la fecha
df['tweet_created'] = pd.to_datetime(df['tweet_created'], format='%Y-%m-%d %H:%M:%S %z')
df['date'] = df['tweet_created'].dt.date

"""# Divisi√≥n del Conjunto de Datos en Entrenamiento y Validaci√≥n"""

df['date'] = pd.to_datetime(df['date'])

# ---- Fecha de corte para test ----
fecha_corte = pd.to_datetime("2015-02-24")

# ---- TEST basado en fecha ----
df_test = df[df['date'] >= fecha_corte]
df_trainval = df[df['date'] < fecha_corte]

# X e y para train/validaci√≥n
X_trainval = df_trainval[['final_text', 'airline', 'date', 'negativereason']]
y_trainval = df_trainval['airline_sentiment']

# ---- Hacer split estratificado SOLO en train+valid ----
X_trn, X_valid, y_trn, y_valid = train_test_split(
    X_trainval,
    y_trainval,
    test_size=0.20,
    random_state=42,
    stratify=y_trainval
)

# ---- Obtener TEST final ----
X_test = df_test[['final_text', 'airline', 'date','negativereason', 'text']]
y_test = df_test['airline_sentiment']

# Vemos tama√±os
print("Train:", X_trn.shape)
print("Valid:", X_valid.shape)
print("Test:", X_test.shape)

X_trn.shape, X_valid.shape, y_trn.shape, y_valid.shape, X_test.shape, y_test.shape

"""# An√°lisis Exploratorio"""

df_train = pd.DataFrame({
    'text': X_trn['final_text'],
    'airline': X_trn['airline'],
    'date': X_trn['date'],
    'negatvieason': X_trn['negativereason'],
    'label': y_trn.values
})

plt.figure(figsize=(10,5))

ax = (df_train['airline']
      .value_counts()
      .sort_values(ascending=False)
      .plot(kind='bar', color='white', edgecolor='black'))

# aplicamos patr√≥n a cada barra
patterns = ['.', '///', '\\\\', 'xx', 'oo', '...']
bars = ax.patches

for bar, pattern in zip(bars, patterns * (len(bars)//len(patterns) + 1)):
    bar.set_hatch(pattern)
    bar.set_edgecolor('black')
    bar.set_linewidth(1.2)

plt.title("N√∫mero de tweets por aerol√≠nea")
plt.xlabel("Aerol√≠nea")
plt.ylabel("Cantidad de tweets")
plt.tight_layout()
plt.show()

# Gr√°fica de Pie
a = df_train.groupby("airline")["label"].value_counts().unstack(fill_value=0)

# Patrones para cada slice
patterns = ['.', '///', '\\\\', 'xx', 'oo', '...']

fig, axes = plt.subplots(2, 3, figsize=(15, 8))
axes = axes.flatten()

for i, ax in zip(range(len(a)), axes):
    temp = a.iloc[i]

    wedges, texts, autotexts = ax.pie(
        x=temp,
        labels=temp.index,
        autopct='%1.1f%%',
        explode=[0.08, 0.03, 0.03],
        colors=["white"] * len(temp),
        textprops={'color': 'black', 'fontsize': 10}
    )


    for wedge, pattern in zip(wedges, patterns):
        wedge.set_facecolor("white")
        wedge.set_hatch(pattern)
        wedge.set_edgecolor("black")
        wedge.set_linewidth(1.3)


    for autotext in autotexts:
        autotext.set_color("black")
        autotext.set_fontsize(11)
        autotext.set_fontweight("bold")


    for text in texts:
        text.set_color("black")
        text.set_fontsize(10)

    ax.set_title(f"{a.index[i]}: {format(a.values[i].sum(), ',')}", fontsize=12)

plt.suptitle("Proporci√≥n de Sentimiento", fontsize=25)
plt.tight_layout()
plt.show()

# Word Cloud por sentimiento
text_neg = " ".join(df_train[df_train['label'] == 'negative']['text'].astype(str).tolist())
text_neu = " ".join(df_train[df_train['label'] == 'neutral']['text'].astype(str).tolist())
text_pos = " ".join(df_train[df_train['label'] == 'positive']['text'].astype(str).tolist())

wc_neg = create_wc(text_neg)
wc_neu = create_wc(text_neu)
wc_pos = create_wc(text_pos)


fig, axes = plt.subplots(1, 3, figsize=(20, 7))


axes[0].imshow(wc_neg, interpolation='bilinear')
axes[1].imshow(wc_neu, interpolation='bilinear')
axes[2].imshow(wc_pos, interpolation='bilinear')

# T√≠tulos
axes[0].set_title("Sentimiento Negativo", fontsize=16)
axes[1].set_title("Sentimiento Neutral", fontsize=16)
axes[2].set_title("Sentimiento Positivo", fontsize=16)


for ax in axes:
    ax.axis("off")


fig.lines.append(
    plt.Line2D(
        [1/3, 1/3],   # posici√≥n x
        [0.05, 0.95], # alto
        transform=fig.transFigure,
        color="black",
        linewidth=2
    )
)


fig.lines.append(
    plt.Line2D(
        [2/3, 2/3],
        [0.05, 0.95],
        transform=fig.transFigure,
        color="black",
        linewidth=2
    )
)

plt.tight_layout()
plt.show()

# Agrupamos por fecha, aerol√≠nea y sentimiento
grouped = (
    df_train
    .groupby(['date', 'airline', 'label'])
    .size()
    .reset_index(name='count')
)

# Serie de tiempo del sentmiento de las aerol√≠neas
labels_order = ['negative', 'neutral', 'positive']

airlines = sorted(df_train['airline'].unique())
n_airlines = len(airlines)

ncols = 3
nrows = int(np.ceil(n_airlines / ncols))

plt.figure(figsize=(5*ncols, 4*nrows))

for i, airline in enumerate(airlines, start=1):
    ax = plt.subplot(nrows, ncols, i)


    temp = grouped[grouped['airline'] == airline]


    temp_pivot = (
        temp
        .pivot(index='date', columns='label', values='count')
        .reindex(columns=labels_order)   # orden fijo
        .fillna(0)
    )


    if 'negative' in temp_pivot.columns:
        ax.plot(
            temp_pivot.index,
            temp_pivot['negative'],
            label='negative',
            color='black',
            linestyle='-',
            linewidth=1.8
        )
    if 'neutral' in temp_pivot.columns:
        ax.plot(
            temp_pivot.index,
            temp_pivot['neutral'],
            label='neutral',
            color='gray',
            linestyle='--',
            linewidth=1.8
        )
    if 'positive' in temp_pivot.columns:
        ax.plot(
            temp_pivot.index,
            temp_pivot['positive'],
            label='positive',
            color='#505050',
            linestyle=':',
            linewidth=2
        )

    ax.set_title(airline)
    ax.set_xlabel("Fecha")
    ax.set_ylabel("N√∫mero de tweets")
    ax.tick_params(axis='x', rotation=45)


    if i == 1:
        ax.legend(title="Sentimiento", loc='upper right')

plt.suptitle("Evoluci√≥n temporal del sentimiento por aerol√≠nea", fontsize=18)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""# Remuestreo"""

X_trn = df_train[['text', 'negatvieason', 'airline']]
y_trn = df_train['label']

# X_trn

# Unimos X_trn y y_trn en un DataFrame
train_df = pd.DataFrame({
    'text': X_trn['text'].values,
    'negatvieason': X_trn['negatvieason'].values,
    'airline': X_trn['airline'].values,
    'label': y_trn.values
})

#  Contamos cu√°ntos ejemplos hay por clase
counts = train_df['label'].value_counts()
max_count = counts.max()

print("Distribuci√≥n original del train:")
print(counts)

#  Hacemos oversampling clase por clase
dfs_balanced = []

for label, count in counts.items():
    df_class = train_df[train_df['label'] == label]

    if count < max_count:
        df_over = df_class.sample(max_count - count, replace=True, random_state=42)
        df_class = pd.concat([df_class, df_over], axis=0)

    dfs_balanced.append(df_class)

# Unimos y mezclamos
train_df_balanced = pd.concat(dfs_balanced, axis=0).sample(frac=1, random_state=42)

print("\nDistribuci√≥n balanceada:")
print(train_df_balanced['label'].value_counts())

# Extraemos tus nuevos sets balanceados
X_trn_bal = train_df_balanced[['text', 'negatvieason', 'airline']].reset_index(drop=True)
y_trn_bal = train_df_balanced['label'].reset_index(drop=True)

"""# Modelaci√≥n Sentimiento

## Modelos de ML
"""

# usamos TfidfVectorizer para transformar los vectores de entrada al modelo
max_features = 5000
tfidf = TfidfVectorizer(
        max_features=max_features,
        ngram_range=(1, 2),
        min_df=2,
        stop_words='english'
    )
X_tfidf = tfidf.fit_transform(X_trn_bal["text"].astype(str))
X_valid_tfidf = tfidf.transform(X_valid['final_text'].astype(str))
X_test_tfidf = tfidf.transform(X_test['final_text'].astype(str))

# CF para encontrar el mejor random forest
rf = RandomForestClassifier(
    random_state=42,
    max_depth=None
)

# Definir la malla del n√∫mero de √°rboles
param_grid = {
    'n_estimators': [40, 60, 80, 100, 120, 200, 300]
}

# GridSearchCV con 3-fold CV
grid = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

grid.fit(X_tfidf, y_trn_bal)

print("\nMejores hiperpar√°metros:", grid.best_params_)
print("Mejor accuracy promedio (CV):", grid.best_score_)

mean_scores = grid.cv_results_['mean_test_score']
std_scores  = grid.cv_results_['std_test_score']
n_estimators_list = param_grid['n_estimators']

print("\nResultados por n√∫mero de √°rboles:")
for n, m, s in zip(n_estimators_list, mean_scores, std_scores):
    print(f"n_estimators = {n:3d} ‚Üí accuracy medio = {m:.4f} ¬± {s:.4f}")

#  Gr√°fica del codo
plt.figure(figsize=(8, 5))
plt.plot(n_estimators_list, mean_scores, marker='o')
plt.fill_between(
    n_estimators_list,
    mean_scores - std_scores,
    mean_scores + std_scores,
    alpha=0.2
)
plt.xlabel("N√∫mero de √°rboles (n_estimators)")
plt.ylabel("Accuracy promedio (CV)")
plt.title("Gr√°fica del codo para Random Forest")
plt.grid(True)
plt.show()

# Vemos las variables m√°s importantes

best_rf = grid.best_estimator_

feature_importances = best_rf.feature_importances_
feature_names = np.array(tfidf.get_feature_names_out())

# ordenar de mayor a menor
indices = np.argsort(feature_importances)[::-1]
top_n = 10

top_features = feature_names[indices[:top_n]]
top_values   = feature_importances[indices[:top_n]]

plt.style.use('seaborn-v0_8-white')

fig, ax = plt.subplots(figsize=(9, 6))


ax.barh(
    y=np.arange(top_n),
    width=top_values[::-1],
    color='black'
)


ax.set_yticks(np.arange(top_n))
ax.set_yticklabels(top_features[::-1], fontsize=12)


ax.set_xlabel("Importancia", fontsize=13)
ax.set_title("Top 10 Variables M√°s Importantes (Random Forest)", fontsize=15)

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.xaxis.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.7)

plt.tight_layout()
plt.show()

# --- M√©tricas del RF ---
print("\n--- RANDOM FOREST ---")
y_pred_rf = best_rf.predict(X_valid_tfidf)
print(classification_report(y_valid, y_pred_rf))
print(confusion_matrix(y_valid, y_pred_rf))

# Mtriz de confuci√≥n del RF
y_pred = best_rf.predict(X_valid_tfidf)

# Confusion matrix
cm = confusion_matrix(y_valid, y_pred)


class_names = ["negativo", "neutral", "positivo"]

plt.figure(figsize=(8,6))
sns.heatmap(
cm,
cmap=plt.cm.Blues,
annot=True,
fmt='d',
xticklabels=class_names,
yticklabels=class_names
)

plt.title('Confusion Matrix - Random Forest', fontsize=16)
plt.xlabel('Predicted label', fontsize=12)
plt.ylabel('True label', fontsize=12)
plt.show()

"""## LSTM"""

# Aplicamos la tokenizaci√≥n y el padding
y_trn = pd.get_dummies(y_trn_bal)
X_trn, tokenizer = tokenize_pad_sequences(X_trn_bal['text'])
y_vld = pd.get_dummies(y_valid)
X_vld = tokenize_valid_set(X_valid['final_text'], tokenizer)
y_tst = pd.get_dummies(y_test)
X_tst = tokenize_valid_set(X_test['final_text'], tokenizer)

# Tama√±o de los Sets de Datos
print('Train:         ', X_trn.shape, y_trn.shape)
print('Validation Set:', X_vld.shape, y_vld.shape)
print('Validation Set:', X_tst.shape, y_tst.shape)

# extraemos las palabras de cada tweet siempre y cu√°ndo esten en el diccionario que hemos creado con tokenizer
word_index = tokenizer.word_index
index_word = {i: w for w, i in word_index.items()}

sentences = []
for seq in X_trn:
    # quitamos los ceros (padding)
    words = [index_word[idx] for idx in seq if idx != 0 and idx in index_word]
    if words:
        sentences.append(words)

# DEfine los par√°metros del embedding
embedding_dim = 100  #

w2v_model = Word2Vec(
    sentences=sentences,
    vector_size=embedding_dim,
    window=5,
    min_count=1,
    sg=1
)

# Asigna los embeddings
vocab_size = 10000
num_words = min(vocab_size, len(word_index) + 1)

embedding_matrix = np.zeros((num_words, embedding_dim))

for word, i in word_index.items():
    if i >= num_words:
        continue
    if word in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word]
    # si no est√°, se queda el vector 0

# Definimos que s√≥lo utilizaremos las 10000 palabras m√°s mencionadas
vocab_size = 10000
# Definimosel tama√±o del ambedding
embedding_size = 100

# Definimos la arquitectura de la Red
model = Sequential()
model.add(
    Embedding(
        input_dim=num_words,
        output_dim=embedding_dim,
        weights=[embedding_matrix],
        input_length=max_len,
        trainable=True   # Este par√°metro le permite a la matriz de embeddings actualizarse
    )
)

model.add(Bidirectional(LSTM(32)))

model.add(Dense(3, activation='softmax'))

model.build(input_shape=(None, max_len))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Visualizaci√≥n del Modelo
plot_model(
    model,
    to_file='modelo_lstm.png',
    show_shapes=True,
    show_layer_names=True,
    dpi=160,
    expand_nested=False
)

# Definimos el Early Stopping
es = EarlyStopping(monitor = 'val_loss', patience=3, restore_best_weights=True)
batch_size = 64
epochs = 30

# Entrenamos el modelo
history = model.fit(X_trn, y_trn,
                    validation_data=(X_vld, y_vld),
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    callbacks = [es])

"""### M√©tricas"""

# M√©tricas del modelo RF
y_pred_probs = model.predict(X_vld)
y_pred = np.argmax(y_pred_probs, axis=1)

# Convertir y_vld one-hot a etiquetas
y_true = np.argmax(y_vld.values, axis=1)

# ----------- Accuracy -----------
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy en validaci√≥n: {accuracy:.4f}")

# ----------- Reporte de clasificaci√≥n -----------
print("\nReporte de clasificaci√≥n:")
print(classification_report(y_true, y_pred, target_names=['negativo','neutral','positivo']))

# ----------- Matriz de confusi√≥n -----------
print("Matriz de confusi√≥n:")
print(confusion_matrix(y_true, y_pred))

# Curvas de aprendizaje modelo recurrente
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], 'b--', label = 'loss')
plt.plot(history.history['val_loss'], 'r:', label = 'val_loss')
plt.xlabel('Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], 'b--', label = 'acc')
plt.plot(history.history['val_accuracy'], 'r:', label = 'val_acc')
plt.xlabel('Epochs')
plt.legend()

plt.show()

# confusion matriz LSTM
plot_confusion_matrix(model, X_vld, y_vld)

"""# Evaluaci√≥n en Datos nunca vistos

## LSTM
"""

y_pred_probs = model.predict(X_tst)
y_pred = np.argmax(y_pred_probs, axis=1)

# Convertir y_vld one-hot a etiquetas
y_true = np.argmax(y_tst.values, axis=1)

df_noseendata_lstm = X_test.copy()
df_noseendata_lstm['true_sentimen'] = y_true
df_noseendata_lstm['pred_sentiment'] = y_pred

x_test_sin_pred = X_test.reset_index(drop =True)
x_test_w_pred = df_noseendata_lstm.reset_index(drop =True)[['true_sentimen', 'pred_sentiment']]

# muestra de como predice el modelo
x_test_sin_pred.join(x_test_w_pred)[['text', 'true_sentimen', 'pred_sentiment']]

a = (
    df_noseendata_lstm
    .groupby("airline")["pred_sentiment"]
    .value_counts()
    .unstack(fill_value=0)
    .reindex(columns=[0, 1, 2], fill_value=0)
)

#  Patrones para cada slice (una por sentimiento)
patterns = ['.', '///', '\\\\']  # 0: neg, 1: neu, 2: pos

#  Etiquetas de las clases (para el pie)
sentiment_labels = {
    0: "negative",
    1: "neutral",
    2: "positive"
}

#  Crear subplots
n_airlines = len(a)
n_rows = 2
n_cols = 3  # asumiendo 6 aerol√≠neas; ajusta si son m√°s/menos

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 8))
axes = axes.flatten()

for i, ax in zip(range(n_airlines), axes):
    temp = a.iloc[i]

    # etiquetas de texto seg√∫n el √≠ndice
    labels = [sentiment_labels[idx] for idx in temp.index]

    wedges, texts, autotexts = ax.pie(
        x=temp.values,
        labels=labels,
        autopct='%1.1f%%',
        explode=[0.08, 0.03, 0.03],
        colors=["white"] * len(temp),  # fondo blanco
        textprops={'color': 'black', 'fontsize': 10}
    )

    # aplicar patrones
    for wedge, pattern in zip(wedges, patterns):
        wedge.set_facecolor("white")
        wedge.set_hatch(pattern)
        wedge.set_edgecolor("black")
        wedge.set_linewidth(1.3)

    # mejorar porcentajes
    for autotext in autotexts:
        autotext.set_color("black")
        autotext.set_fontsize(11)
        autotext.set_fontweight("bold")

    # mejorar labels
    for text in texts:
        text.set_color("black")
        text.set_fontsize(10)

    # t√≠tulo de cada subplot: nombre de aerol√≠nea + total de ejemplos
    ax.set_title(f"{a.index[i]}: {temp.sum():,}", fontsize=12)

for j in range(n_airlines, len(axes)):
    axes[j].axis('off')

plt.suptitle("Proporci√≥n de predicciones de sentimiento por aerol√≠nea", fontsize=20)
plt.tight_layout()
plt.show()